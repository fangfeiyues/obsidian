## 1、Redis

### 实现方案
1. 先设置 `setnx (set if not exists)` 结束了再 `del`，可能发生异常导致没调用 del指令
2.  expire timeout。但两个指令之间不是原子性的可能中间出问题
3.  `set lock:code true wx 5 nx` 命令提供原子性操作

```bash
> set lock:codehole true ex 5 nx
OK
-- ... do something critical ...
> del lock:codehole
```

### 超时问题

-  **问题**
	 如果逻辑执行太长以至于超出了锁的超时限制，第一个线程持有的锁过期但逻辑还没执行完，同时第二个线程就提前重新持有这把锁，导致临界区的代码不能严格串行。

- **方案**
	有个稍微安全的方案是为set指令的value参数设置一个随机数，释放锁时先匹配随机数是否一致然后再删除key，这是为了确保当前线程占有的锁不会被其他线程释放，除非是这个锁过期被服务器自动释放。

### 可重入性

利用 ThreadLocal<Map<String, Integer>> 识别当前线程是否存在

## 2、Bond

### 实现方案

- **一期**
	1、偏向强一致性（CP），影响就是某单个节点发生网络延迟等就不能读写，但能保证所有节点数据一致
	2、etcd的有序节点+监听器方案， 实现是 Server 端实现，对于 Client 端就只是一个 Lock 指令
	

- **二期**
	1、可用性 > 强一致性（AP），影响就是允许Server端延迟时C&S的不一致，但能带来非常大的性能提升?
	2、Aerospike的副本方案 ???

### Q1：加锁-可重入

-  **场景**
	同一个线程中，经过不同的类，有多处地方进行同一把锁的加锁请求

-  **方案**
	线程本地变量ThreadLocal存储当前的状态
	1、加锁则根据 key 判断是否为新调用，如果是则执行实际加锁请求，否则则重入计数器 +1 
	2、解锁则重入计数器先 -1 ，再判断是否小于 0 ，是的话则执行真实的解锁请求
### Q2：加锁-超时后重试重入

-  **场景**
	![[image-Distributed-1 分布式锁-20240418195457231.png|600]]

-  **实现方案**
	简单来说就是在加锁超时后，再次去获取锁，如果获取到则说明加锁成功否则加锁失败

-  **不足**
	 -->网络原因超时 
	 --> 发起第二次加锁 
	 --> 这时第一次成功 
	 --> 则第二次失败，就存在lease time没有任何线程拿到锁
	 解决方案就可以考虑二次加锁之前进行重入查询

### Q3：解锁-线程隔离

-  **场景**
	线程A到释放时间但业务逻辑还未执行完，同时过后执行释放锁命令，这时线程之间存在互相unlock的情况
	![[image-Distributed-1 分布式锁-20240418200506311.png]]


-  **方案**
	- 发送 lock 请求之前记录时间点 `T`
	- 执行 unlock 操作的时候
	    1.  若当前时间 <= ( T + leaseTime - 150ms ) ，发送 unlock 请求。
	    2.  若当前时间 > ( T + leaseTime -150ms ) && 当前时间 < ( T + leaseTime ) ， 不执行任何操作
	    3.  若当前时间已经超时，则不发送 unlock 请求，且 告警 给对应的应用 owner 
	  (150ms 是给予 unlock 操作的缓冲时间段，即从发送请求开始到 server 执行unlock操作的时间段) 


## 3、Redlock

```
  为了解决在Sentinel集群中，主节点挂掉时从节点会取而代之但可能之前节点A没能把锁信息同步给主节点就挂了，这样就会有两把锁节点。

  加锁时，它会向过半节点发送 `set(key, value, nx=True, ex=xxx)` 指令，只要过半节点 `set`成功，那就认为加锁成功。释放锁时，需要向所有节点发送 `del`指令。但Redlock 算法还需要考虑出错重试、时钟漂移等很多细节问题，同时因为 Redlock 需要向多个节点进行读写，意味着相比单实例 Redis 性能会下降一些。
```



[有赞 Bond 分布式锁](https://mp.weixin.qq.com/s/X7e0W5GCul3DrnuPu9aoUg)