
-  **Redis is AP or CP?**

	**AP** 。Redis设计的目标是*高性能、高可扩展和高可复用性*，Redis的一致性模型是最终一致性，没法保证强一致性的原因是它分布式设计采用的是异步复制，导致节点之间存在的数据同步延迟和不一致的可能
	[[Distributed-1  分布式基论·CAP]]


Redis集群同步的3种方式：主从、哨兵、cluster

## 3.1  主从同步 -> 数据复制

	Redis 的复制功能分为`同步(sync)` 和 `命令传播(command propagate) `
	-  `同步操作` ：用于从服务器的数据库状态 --> 主服务器当前所处的数据库状态
	-  `命令传播` ：用于在主服务器的数据库状态被修改，导致主从服务器不一致时，让主从回到一致

### 作用

1.  **数据冗余**：主从同步实现了数据的热备份
2.  **故障恢复**：从节点 --> 主节点，通常会使用 Sentinel 哨兵实现自动转移
3.  **负载均衡**：从节点分担读服务
4.  **高可用基石**：主从复制是 哨兵 和集群 的基石

### 过程

-  **旧版同步**

	简单总结旧版的 `SYNC命令` 是一个非常耗费资源的操作（ Redis 2.8前 ）
	-  主服务器 需要执行`BGSAVE` 命令来生成 `RDB文件`，这个操作会耗费主服务器大量的 **CPU、内存 和 磁盘I/O**
	-  主服务器 需要将自己生成的 RDB文件 发送给 从服务器，也会耗费大量的网络资源，并对主服务器响应命令请求时间产生影响
	-  从服务器 接收到 RDB 文件 需要载入，并在载入期间从服务器会因为 阻塞 而不能处理命令请求
	  
	  ![[Redis-3 分布式.png|500]]


-  **新版同步**

	Redis 从 2.8版本 开始使用 `PSYNC命令` 代替 `SYNC命令` 来执行复制时同步
	`PSYNC命令`具有 `完整重同步(full resynchronization) ` 和 `部分重同步(partial resynchronization)`


-  **部分重同步**

	主要由三个部分构成：

	1、`复制偏移量 replication offset`
		执行复制的双方主从服务器分别会维护一个 复制偏移量 以来决定开始复制的位置
			1、主服务器每次向从服务器传播N个字节数据，就将自己的复制偏移值加上N
			2、从服务器每次收到主传来的N个字节数据，也将自己的复制偏移量值加上N
			
		![[Redis-3 集群-部分重同步-复制偏移量.png|400]]
		
		
	2、`复制积压缓冲区 repl_backing_buffer`
		是主服务器维护的一个固定长度先进先出队列，默认大小为1MB。保存着最近传播的写入命令，并且会为队列每个字节记录相应的复制偏移量，以决定是部分同步还是全量同步
		1.  如果 offset 偏移量之后的数据还在复制缓冲区，那么对从服务器执行部分重同步
		2.  如果 offset 偏移量不在缓冲区，那么还是执行完整重同步
		   
	  `repl_backlog_buffer`：它是为了从库断开之后，如何找到主从差异数据而设计的环形缓冲区，从而避免全量复制带来的性能开销。如果从库断开时间太久，repl_backlog_buffer 环形缓冲区被主库的写命令覆盖了，那么从库连上主库后只能乖乖地进行一次全量复制，所以 **repl_backlog_buffer配置尽量大一些，可以降低主从断开后全量复制的概率**。而在 repl_backlog_buffer 中找主从差异的数据后，如何发给从库呢？这就用到了replication buffer。
		
	  `replication buffer`：Redis和客户端通信也好，和从库通信也好，Redis都需要给分配一个 内存buffer进行数据交互，客户端是一个client，从库也是一个client，我们每个client连上Redis后，Redis都会分配一个client buffer，所有数据交互都是通过这个buffer进行的：Redis先把数据写到这个buffer中，然后再把buffer 中的数据发到 client socket 中再通过网络发送出去，这样就完成了数据交互。所以主从在增量同步时，从库作为一个 client，也会分配一个 buffer，只不过这个 buffer 专门用来传播用户的写命令到从库，保证主从数据一致，我们通常把它叫做replication buffer。
	  
	3、`服务器运行ID`
	主服务器 根据 来同步的从服务器ID 决定之前是否同步过


### 拓扑

1.  一主一从
2.  一主多从
3.  树状主从结构

### 存在问题

-  **人工高可用**
  
	一旦主节点出现故障，需要手动将一个从节点晋升为主节点，同时需要修改应用方的主节点地址，还需要命令其他从节点去复制新的主节点，整个过程都需要人工干预

-  **分布式问题**

	1.  主节点的写能力受到单机的限制
	2.  主节点的存储能力受到单机的限制

## 3.2 Sentinel 哨兵 -> 故障转移

Sentinel 哨兵是 Redis 的高可用性解决方案：由一个或多个Sentinel组成的系统可以监视任意多个主服务器，以及这些主下属的从，并在被监视的主下线时自动将下线主属下的某个从升级为新的主。

	![[image-Redis-3 分布式-20240625013240882.png|600]]

### 哨兵作用

-  **1、获取主从服务器信息**

	Sentinel 默认会以每十秒一次的频率，通过命令连接向被监视的主发送 INFO 命令获取主当前信息
	-  主本身的信息，包括 run_id 域记录的服务器运行ID，以及 role 域记录的服务器角色
	-  主下属所有从信息，每个从由 “slave” 字符串开头的行记录

-  **2、检测主观下线状态**

	Sentinel 会默认以每秒一次的频率，向所有与它创建命令连接的实例如主从其他Sentinel等发送 PING命令，并通过PING回复来判断实例是否在线，配置 `down-after-millseconds = 50000ms` 那么当 master 连续 50000毫秒 返回无效回复时，Sentinel 就会将 master 标记为主观下线

-  **3、检测客观下线状态**

	Sentinel将一个主判断为主观下线后，为了确认是否真的下线它会同样向其他Sentinel询问，如果接受的数量足够多，Sentinel就会判断为客观下线并对主进行故障转移


-  **4、选举领头Sentinel**

	Sentinel 设置局部零头的规则是 **先到先得**：最先向目标 Sentinel 发送 `is-master-down-by-addr`设置要求的，而之后接受到所有设置都会被目标 Sentinel 拒绝。得到半数以上的Sentinel将成为领头，否则将再次选举，就是一个 `Raft选举算法` （详见： [[Distributed-4 分布式一致性算法#Raft]]）


-  **5、故障转移**

	1. 选出新的主服务器
		`SLAVEOF no one 将选中的从服务器转换为主服务器。（新的主数据不完备怎么办？）
	2.  Step2：修改从服务器的复制目标
	3.  Step3：将旧的主变为从服务器


### 领导者Sentinel选举

	![[image-Redis-3 分布式-20240625013904873.png|600]]

1. 每个在线的 Sentinel 节点都有资格成为领导者，当它确认主节点主观 下线时候，会向其他 Sentinel 节点发送 sentinel is-master-down-by-addr 命令， 要求将自己设置为领导者。
2. 收到命令的 Sentinel 节点，如果没有同意过其他 Sentinel 节点的 sentinel is-master-down-by-addr 命令，将同意该请求，否则拒绝。
3. 如果该 Sentinel 节点发现自己的票数已经大于等于 max（quorum， num（sentinels）/2+1），那么它将成为领导者。
4. 如果此过程没有选举出领导者，将进入下一次选举。

### 新主节点挑选


	![[image-Redis-3 分布式-20240625013849650.png|500]]

1. 过滤：“不健康”（主观下线、断线）、5 秒内没有回复过 Sentinel 节 点 ping 响应、与主节点失联超过 down-after-milliseconds*10 秒。
2. 选择 slave-priority（从节点优先级）最高的从节点列表，如果存在则返回，不存在则继续。
3. 选择复制偏移量最大的从节点（复制的最完整），如果存在则返 回，不存在则继续。
4. 选择 runid 最小的从节点。

## 3.3 Cluster 集群 -> 自动发现

### 作用

	![[image-Redis-3 分布式-20240625014143549.png]]

1. Redis Cluster 是 Redis 推荐的分布式解决方案，它将数据自动分片到多个节点，每个节点负责一部分

2. Redis Cluster 采用主从复制来提高可用性，每个分片都有一个主节点和多个从节点，主节点负责写从节点负责复制数据并处理读请求

3. Redis Cluster 能自动检测节点故障，可标记不可用 或 从升主等


### 数据分片

Redis的Cluster集群模式中，使用哈希槽（hash slot）方式来进行数据分片，先将整个数据集划分为多个槽，每个槽分配一个节点

![[image-Redis-3 集群-20240605005202697.png|600]]

整个数据集划分为 16384 ( = 2^14 ) 个槽，集群每个节点可以划分多个槽。

当需要存储或检索一个键值对时，Redis Cluster 会先计算这个键的哈希槽，然后找到负责这个哈希槽的 Redis 实例，最后在这个实例上进行操作


-  **取余分区**

	节点取余分区是一种简单的分区策略，其中数据项通过对某个值（通常是键的哈希值）进行取余操作来分配到不同的节点。
	
	类似 HashMap 中的取余操作，数据项的键经过哈希函数计算后，对节点数量取余，然后将数据项分配到余数对应的节点上。
	
	缺点是扩缩容时，大多数数据需要重新分配，因为节点总数的改变会影响取余结果，这可能导致大量数据迁移。

-  **一致性哈希分区**

	一致性哈希分区的原理是：将哈希值空间组织成一个环，数据项和节点都映射到这个环上。数据项由其哈希值直接映射到环上，然后顺时针分配到遇到的第一个节点。
	
	从而来减少节点变动时数据迁移的量
	
	![[image-Redis-3 分布式-20240625015959234.png|500]]
	
	这种方式相比节点取余最大的好处在于加入和删除节点只影响哈希环中相邻的节点，对其他节点无影响
	
	但它还是存在问题：
	- 节点在圆环上分布不平均，会造成部分缓存节点的压力较大
	- 当某个节点故障时，这个节点所要承担的所有访问都会被顺移到另一个节点，对后面这个节点造成压力


-  **虚拟槽分区**

	在虚拟槽（也叫哈希槽）分区中，存在固定数量的槽位（例如 Redis Cluster 有 16384 个槽），每个键通过哈希算法（CRC16）映射到这些槽上，每个集群节点负责管理一定范围内的槽。
	
	这种分区可以灵活地将槽（以及槽中的数据）从一个节点迁移到另一个节点，从而实现平滑扩容和缩容；数据分布也更加均匀，Redis Cluster 采用的正是这种分区方式
	
	![[image-Redis-3 分布式-20240625020254052.png|600]]
	
	
	伸缩？ -- 集群扩容和缩容的关键点，就在于槽和节点的对应关系，扩容和缩容就是将一部分`槽`和`数据`迁移给新节点



## 分布式锁

### SETNX

-  **实现**

	1.  **tryLock：**`jedisPool.set(key, requestId, "NX", "PX", expressTime)`
	2.  **unlock：** lua脚本判断值是否等于再删除 `jedisPool.eval(script, key, requestId)` -- 
	![[image-Redis-3 分布式-20240626020222462.png]]


-  **优点**

	1.  实现简单，SETNX命令实现简单，易于理解和使用
	2.  性能较高，SETNX命令执行原子性，保证了分布式锁的正确性；且是单线程执行

-  **缺点**

1.  锁无法续期，如果加锁方的执行时间较长，而锁的时间设置较短，可能导致锁被误解
2.  无法避免死锁
3.  存在竞争
4.  SETNX不支持重入

### Redission

1.  支持可重入
2.  支持多个读锁和一个写锁对同一个资源加锁
3.  支持 RedLock，解决单点问题

-  **RedLock**

	![[image-Redis-3 分布式-20240626021738740.png]]
	
	
	这样当超过半数以上的节点都写入成功后，及时master挂了，新选出来的master也能保证刚刚那个key存在，否则不会选为master
	
	但也不能完全解决分布式锁问题，例如在脑裂的情况下，可能会产生两个客户端同时持有锁（分区了，节点间不能相互交流）


