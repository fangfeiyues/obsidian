
	RL: Reinforcent Learning (强化学习)
	CoT: Chain of Thoughts (思维链)
	SFT: Supervised Fine-Tuning (监督微调)

##  模型差异

- **推理模型（理科生）**

	是指在传统大语言模型的基础上，`强化推理、逻辑分析 和 决策能力`的模型，他们通常具备额外的技术比如强化学习、神经符号推理、元学习等，来增强其推理和问题解决能力。
	
	例如 DeepSeek-R1、GPT-o3 在逻辑推理、数学推理和实时问题解决方面表现突出


-  **非推理模型（文科生）**

	适用于大多数任务，非推理大模型一般侧重于`语言生成、上下文理解 和 自然语言处理`，而不强调深度推理能力。此类模型通常通过对大量文本数据的训练，掌握语言规律并能够生成合适的内容，但缺乏像推理模型那样复杂的推理和决策能力
	
	例如：GPT-3、GPT-4（OpenAI），BERT（Google），主要用于语言生成、语言理解、文本分类、翻译等任务
	![[image-DeepSeek-20250212163838671.png]]


## 提示语策略


-  **推理模型**

	1.  提示语更简洁，只需明确任务目标和需求，因其已内化推理逻辑
	2.  无需逐步指导，模型自动生成结构化推理过程，若强行拆解步骤反而限制其能力


-  **非推理模型**

	1.  需显示引导推理步骤，如通过Cot提示，否则可能跳过关键步骤
	2.  依赖提示语补偿能力短板，如要求分步思考、提供示例


![[image-DeepSeek-20250213114712463.png]]



## 提示句场景

-  **文案写作**

-  **营销策划**

-  **品牌故事**







