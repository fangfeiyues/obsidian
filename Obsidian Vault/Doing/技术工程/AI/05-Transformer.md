## 框架结构

![[Pasted image 20250313170302.png|400]]

## Tokenization：文本 -> Token

把一段文字变成一组Token（1000个tokens大概750英文，500汉子）

1.  2020年 GPT-3，Token词表有50257个
2.  ...

## Embedding：Token -> 向量

向量、空间、特征

什么是向量：一维向量坐标「3」，二维向量「2，5」，三维向量：[1, 2, 3] ... 

比如有 3000 个汉字，就可以用 3000 维向量表示，如“ 我” 的向量就是 [1,0,0,0,0,.....(2999个)]

但这种方式也存在几个问题
1.  维度过高，过于稀疏。10w个汉字就需要10w维的token
2.  没有体现距离的感念。意思相近则距离更近
3.  没有数学或逻辑概念。最好能满足 国王 - 男人 + 女人 = 女王

Embedding：专门把 Token 映射到新的数学空间中的一个点（向量），
2017年是512维，GPT-3到12288维，Llama 16384
比如“我”，假设是一个token，[1,3,4,-12,23...] 16384维的向量


## Positional Encoding

一句话的语言含义除了取决文字外，还收文字位置的影响。如 “猫咬了狗”，和 “狗咬了猫”
input[0] = embedding[0] + position[0]


## Encoder & Decoder

Seq2Seq: Sequence-to-Sequence

指责：

编码器 -> 向量，解码器 -> “下一个字”

编码器：多头处理机制，并行计算的并行聚合后 每个向量都是被信息聚合的结果（找到向量相似）？
解码器：自回归的过程，输出同时作为输入


### Attention

自注意力机制 ：”请你的注意力放到那些重要的问题上，相关度高的就是你要注意的“

前身：RNN循环神经网络 
1.  信息丢失
2.  无法处理长句子。不能长上下文理解段落
3.  不能并行运算。Attention是自我注意的并行

Attention(Q,K,V) = 

Q K V 的定义和计算：通过 Q 与 K V 的 相关度系数 加权求和后得到一个输出（真正有价值的是 V）

### Self Attention

核心目的是做信息聚合。把一个字的向量价值不断放大，不断的加权求和

自注意力机制。如果 Q K V 都变成了一个段落本身，会怎么样？变成一个二维数组矩阵

问题
1.  整个计算过程没有神经网络。没有任何可学习参数
2.  比较依赖Emdedding向量、位置向量
3.  也会带来一些问题


例子：很多人有很多知识如科学、技术、艺术...  通过矩阵映射出 每个人感兴趣的AI知识，每个人掌握的AI知识，AI知识的加权汇总

2024年Llama-3：126层信息聚合，层数越高向量越长，作为一个中心主体词在不断的抓取聚合信息那么他则越来越大，最终实验成12288维向量。

看一段话的时候，会把1300个词

### Multi-Head Self Attention

多头注意力机制

到16层，多头角度去观察



- 例子

	![[Pasted image 20250313175021.png|400]]