
	几个问题（自上而下）
	1.  大模型为什么火爆，能解决什么问题
	2.  大模型之间核心区别是什么，如 kimi 和 deepseek、openai
	3.  大模型系统核心流程？为什么这么强
	4. ...

## 1、发展史

`AI大模型是一种基于深度学习的超大神经网络模型，通过海量训练数据获得理解和生成能力。它的核心突破在于泛化能力 -- 无需针对特定任务专门训练，就能解决多种复杂问题。` 


GPT 的诞生要归于 NLP 的快速发展，从 2018年 到 2021年，是第一代大语言模型LLM 的技术爆炸期，人们逐渐学会了如何使用海量的无标签数据，来训练这些涌现智能的大模型；随后，OpenAI 采用强化学习技术，点亮了 LLM 的智能，ChatGPT 横空出世

-  **OpenAI 的强化技术**
	1.  插件和联网，祢补大语言模型自身记忆不足
	2.  函数功能，LLM 学会使用API来完成复杂任务
	3.  让 LLM 思考的方法，智能体 Agent 具备自治能力需要将三者结合起来
		1.  任务规划 Planning，不能代替别人思考但可以通过提问引导别人思考就好像产婆引导孕妇
		2.  记忆唤醒 Memory
			1.  感觉记忆：Embedding
			2.  短期记忆：当前意识中的信息，类似于 Prompt
			3.  长期记忆：能回忆的所有信息，类似于外部向量存储
		3.  驾驭工具 tools
		   
		![[image-AI-02 AI大模型-20250210205940439.png|500]]


## 2、底座

大模型接口很简单，像OpenAI 就只提供了 Complete 和 Embedding 两个接口，
1.  Complete 可以让模型根据你的输入自动续写
2.  Embedding 可以将你输入的文本转化成向量

### Complete


### Embedding

这个 API 可以把任何你指定的一段文本，变成一个大语言模型下的向量，也就是用一组固定长度的参数来代表任何一段文本。

比如 “好评” 和 “差评” 两个字的 `Embedding` ，对于任何一段文本评论我们也都可以通过 API 拿到他的 `Embedding`

### llama-index

每次向 AI 提问的时候，都会先去查询下这个第二大脑里面的资料，找到相关资料后，再通过自己的思维能力来回答问题

### LangChain 

LangChain 链式调用粘合剂

LLMChain 对 LangChain 多步的封装

![[image-AI-02 AI大模型-20250209155600840.png|600]]



## 3、AIRC 系统建模

AIRC 内容推荐系统，如抖音的推荐、阿里妈妈计算广告等系统。

AI 在这到底是什么角色？

1.  `数据处理和分析`：AI算法可以处理大量的用户数据和物品数据，提取有用的特征和模式
   
2.  `模型训练和优化`：AI可以训练出高效的推荐模型，不断优化推荐结果，提高推荐的准确性和个性化程度。例如，使用图神经网络（GNN）来捕捉用户和物品之间的复杂关系
   
3.  `实时推荐`：AI能够实现实时推荐，根据用户的实时行为和偏好，快速生成个性化的推荐列表，提升用户体验。例如，通过实时分析用户的点击行为，动态调整推荐内容
   
4.  `风险控制`：AI可以帮助识别和过滤不良内容，检测和防止作弊行为，保障推荐系统的安全性和可靠性。例如，使用自然语言处理（NLP）技术来检测和过滤有害内容

从业务目标到最终的 AIRC 系统，要经过两次转化：首先是产品经理和算法专家将业务目标形式化，这一步将会将业务问题转变为明确的数学问题；在下一步，算法和架构师会将数据问题解成工程问题其主要是召回、排序、控制博弈和风控。   

### 3.1 策略建模

电商系统通常把 GMV指标作为北极星指标，即 “从海量商品中选出此人最可能成交的商品”

-  **如何得到排序的概率值**
  
	期望收益 = 点击率 * 加购率 * 付款率 * 客单价


-  **如何对海量商品进行在线实时排序**
  
	1.  向量召回：基于对比学习和图神经网络，刻画用户和商品之间的空间距离，以此作为召回顺序依据
	2.  排序模块：对召回结果进行精准的打分排序


-  **控制 & 博弈**
  

-  **风控**


### 3.2 特征工程



### 3.3 模型工程

#### 监督学习 Supervised Learning

在正确答案的指导下进行学习，这和考试前通过练习题和答案来对照学习是一样的。

#### 对比学习 Contrastive Learning

对比学习的目标是通过样本之间的相似度，来学习它们之间的距离，进而表示它们的关系

#### 强化学习 Reinforcement Learning

强化学习不依赖预先标记的数据，而是通过与环境的交互来进行自主学习，根据奖励信号的反馈进行实时的策略更新； 其核心思想是利用感知和行动的闭环进行学习


## 4、系统构建

### 模型工程

-  **存量模型**
	![[Pasted image 20250225203521.png]]

-  **增量模型**

	![[Pasted image 20250225203652.png]]

### 存储索引

#### 倒排索引服务

ES

#### 向量索引服务

基于规则的方式很可能会误杀大量候选内容，因此目前最先进的方式是基于语义相关性的向量召回（embedding based retrieval），将用户和物品进行联合建模，得到高纬度投影函数，然后将用户和物品投影到高纬空间，得到坐标，也就是 Embedding。

为了满足在线服务的效率要求，我们通常会预先计算物品和已知用户的向量，并将它们存储在向量引擎中。这样，当用户发起请求时，只需读取用户的向量，并通过向量引擎找到与用户最接近的物品即可


## 5、预训练模型

### CNN

Convolutional Neural Networks 卷积神经网络，视觉算法

### NLP

Nature Language Pre 自然语言预训练

### Word2Vec

Word2Vec算法是 Google 2013年提出，找到一种无监督学习的方法，对大规模的语料库进行预训练，这绕开了对大规模有标签数据集的依赖，打破了NLP预训练数据上的困境，

### BERT

NLP预训练模型 BERT，采用 Encoder Only 的 Tansformer 架构，这一架构能同时利用上文和下文信息，为 BERT 带来显著性能提升


相比计算机视觉（CV）领域的 PTM 技术，NLP 面临更多的困难和复杂性。为了让模型更好地理解人类语言，研究人员一直在探索和突破，这也推动了 NLP 技术的发展。

早期的尝试，如 Word2Vec，为 NLP 预训练模型的发展找到了正确的方向，为后来的研究提供了信心和希望。然而，这些方法也存在着一些明显的缺点，如无法处理词在不同语境中的不同含义。

随后，ELMo 和 GPT 等模型的出现为 NLP 预训练模型的发展带来了突破。ELMo 使用双向的语言模型，将上下文信息融入到 NLP 词向量，更准确地表示了词语的含义。而 GPT 则通过大规模预训练，在各类 NLP 下游任务中取得了显著的成果。

BERT 通过引入双向语言模型和更大规模的预训练数据，在各种 NLP 任务中展现了出色的能力，在工业界取得了巨大的成功，成为了各大公司不可或缺的核心技术之一。

### Transformer

`在此之前，我想先教给你两个理解和掌握复杂事物的方法。首先，你需要自顶向下地去学习，而不要被裹挟到细节的旋涡之中。第二，我们要去拆解事物的每个组件，剖析各个组件的必要性，这样会帮助你理解，为什么它被设计成了现在这个样子。`


### GPT

它的全称是 Generative Pre-trained Transformer，其中集合了我们大模型关注的各种要素，包括预训练大模型（Pre-trained Transformer）和 生成式 AI（Generative AI）。
