

## 框架结构


## Tokenization：「文本」变成「Token」

把一段文字变成一组Token（1000个tokens大概750英文，500汉子）

1.  2020年 GPT-3，Token词表有50257个
2.  ...


## Embedding：「Token」变成「向量」

向量、空间、特征

什么是向量：一维向量坐标「3」，二维向量「2，5」，三维向量：[1, 2, 3] ... 

比如有 3000 个汉字，就可以用 3000 维向量表示，如“ 我” 的向量就是 [1,0,0,0,0,.....(2999个)]

但这种方式也存在几个问题
1.  维度过高，过于稀疏。10w个汉字就需要10w维的token
2.  没有体现距离的感念。意思相近则距离更近
3.  没有数学或逻辑概念。最好能满足 国王 - 男人 + 女人 = 女王

Embedding：专门把 Token 映射到新的数学空间中的一个点（向量），
2017年是512维，GPT-3到12288维，Llama 16384
比如“我”，假设是一个token，[1,3,4,-12,23...] 16384维的向量


## Positional Encoding

一句话的语言含义除了取决文字外，还收文字位置的影响。如 “猫咬了狗”，和 “狗咬了猫”
input[0] = embedding[0] + position[0]



## Encoder & Decoder

指责：

编码器 -> 向量，解码器 -> “下一个字”

编码器：多头处理机制，并行计算的并行聚合后 每个向量都是被信息聚合的结果（找到向量相似）？
解码器：自回归的过程，输出同时作为输入
