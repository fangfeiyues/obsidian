
## Pretrain 预训练

阅读大量大量人类文字，学习人类如何使用文字

m 百万；b 十亿；t 万亿；
一般 Llama3 使用了 15t token，假设2000字一个段落那大概是37.5段落


段落联合概率：一个字一个字出现概率的乘积

LLM：算法  平衡所有 段落联合概率


核心问题：预训练模型说起话来非常像接话茬，并不是在做任务。（不会思考）

## SFT 有监督的微调 

Supervised Fine-Tuning

使用有标签的数据进行训练，学习过程就是有监督学习；如评论 “送货真快” -> 正向标签、相似标签 等

有监督即有正确答案，
自监督即自己的输出作为自己的输入，也叫自回归
半监督：Pretrain + SFT

Instruction Tuning 指令微调，告诉你指令让你干活，其实也是一种监督微调

几十万条数据，大概是预训练阶段的 1/5000，需要的是数据的质量

核心问题：准备了什么数据，就会做什么任务（但要做的更好）


## Reward Model 奖励模型

开始使用LLM生成的数据做训练

评估是一个很难的事情    --> 逐条评估
评估一个模型的整体能力
评估一个问题回答的怎么样


1、准备几万到几十万Prompt，让模型给每个 prompt 生成多个 response





