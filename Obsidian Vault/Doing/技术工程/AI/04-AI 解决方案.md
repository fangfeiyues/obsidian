
## Pretrain 预训练

阅读大量大量人类文字，学习人类如何使用文字

m 百万；b 十亿；t 万亿；
一般 Llama3 使用了 15t token，假设2000字一个段落那大概是37.5段落


段落联合概率：一个字一个字出现概率的乘积

LLM：算法  平衡所有 段落联合概率


核心问题：预训练模型说起话来非常像接话茬，并不是在做任务。（不会思考）

## SFT 有监督的微调 

Supervised Fine-Tuning

使用有标签的数据进行训练，学习过程就是有监督学习；如评论 “送货真快” -> 正向标签、相似标签 等

有监督即有正确答案，
自监督即自己的输出作为自己的输入，也叫自回归
半监督：Pretrain + SFT

Instruction Tuning 指令微调，告诉你指令让你干活，其实也是一种监督微调

几十万条数据，大概是预训练阶段的 1/5000，需要的是数据的质量

核心问题：准备了什么数据，就会做什么任务（但要做的更好）


## Reward Model 奖励模型

开始使用LLM生成的数据做训练，奖励模型不是语言模型，是个单独模型

评估是一个很难的事情    --> 逐条评估
评估一个模型的整体能力
评估一个问题回答的怎么样


1、准备几万到几十万Prompt，让模型给每个 prompt 生成多个 response
2、如何标注，打分？评级？排序
3、找一批人来做标注工作（肯尼亚2美元每天的标注工）
	1.  openAI 提供 a b c d四个选择
	2.  meta llama 提供 a b 并通过 b 生成 偏好数据（更好数据）

4、<Prompt、Response> pair 排序后，打分？


很多专业的问题，怎么标注奖励？



## 强化学习

三部曲：SFT、Reward Model、强化学习

Decision Making 策略

探索 和 跟随：0 -1 的偏向值

步骤：SFT 微调模型后，根据 prompt 生成 response，然后再用 reward model 去打分，PPO 算法排分后去更新策略（反馈给SFT ）并更新 reward model 

