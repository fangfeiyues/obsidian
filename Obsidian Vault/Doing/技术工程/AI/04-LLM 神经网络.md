
## 模型

模型就是一个数学公式，设计模型就是设计能解决真实问题的数学公式。

确定数学公式的过程：

1.  公式，如 y = a x + b
2.  参数，a = 50，b = -100

## 神经网络

模拟人脑，设计一种一劳永逸的公式结构。

如“基于 MNIST 数据库的图像识别”：
包含 70000 张手写数字的图像，每一张都是 28* 28 像素的图像，其中60000张用于训练10000张用于预测
任务是给订一张 28* 28 图像，经过一系列数学公式计算后，输出10个概率，分别代表0-9中某个数字的概率

经过 784 * 785 + 27 * 785 + 10 * 29 = 637710 个公式参数的调整得到最终的概率

![[Pasted image 20250319001613.png|400]]

z0 = x0 * w0 + x1 * w1 + x2 * w2 + ...  + x783 * w783
同理，z1 = x0 * w0 + x1 * w1 + x2 * w2 + ...  + x783 * w783（这里 w0 的值是不一样的）

多层结构可以提取更深层的特征。

## 机器学习

通过计算机完成大规模数学计算，以找到相对更优的参数组合过程就是机器学习，也就是模型训练

神经网络是万能公式，那机器学习呢？

确定数学公式的过程：
1、设计公式：有了神经网络，设计公式结构轻松多了（人类设计的公式融合神经网络）
2、确定参数：人类真的一点办法没有

GPT有多少个参数？GPT-3 和 InstructGPT 都是 1750亿 个参数，GPT-4大概有1.8万亿参数
GPT-3 通过堆叠 96层 每层包含 128 个注意力头的Transformer，构建深层网络，形成复杂的语言表征能力

### 损失函数

如何定义误差？
目标是总误差最小化

如 “定义鳄鱼和蟒蛇的体重计算模型”，怎么通过一个参数组(a,b)来使最终的误差不断减少

![[Pasted image 20250319002202.png|600]]

1、公式： y = ax + b
2、参数：确定 a 和 b 分别是多少

1、随机初始化一组（a, b），比如 a = 3, b = 5
2、在训练数据集中，利用 y = ax + b 进行分类
3、计算分类结果的误差
4、计算 a 和 b 的值应该如何变化可以减小误差
5、计算出一组新的 (a,b) 值 
6、回到步骤2


GPT-3 就是通过 1750亿 个参数的不断调整，以来减少总误差。

